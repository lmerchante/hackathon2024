{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template de entrega\n",
    "\n",
    "El propósito de este template es que la organización del evento pueda ejecutar el/los prompts generados por el grupo sobre el conjunto de preguntas de evaluación que irían en el mismo formato que las que aparecen en este ejemplo.\n",
    "\n",
    "Este documento es sólo una plantilla. Cada grupo puede modificarla de la manera que considere. El único requisito es que el notebook entregado sea capaz de procesar un fichero **\"preguntas.json\"** que tenga el mismo formato que el que aparece en este repositorio. Ya que la evaluación de los prompts se realizará con un fichero **\"preguntas_evaluación.json\"** confidencial que tiene el mismo formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El fichero **\"prompts.json\"** debe incluir uno o varios prompts en caso de que el grupo haya decidido responder al desafío con un único prompt o haya decidido generar un prompt específico para cada módulo del examen de acceso.\n",
    "\n",
    "En el fichero **\"prompts.json\"** proporcionado en este repositorio hemos incluido 5 prompts, uno para cada categoría del examen de acceso. Notar que los 4 primeros prompts son el mismo y el quinto prompt es algo más elaborado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON files\n",
    "with open(\"prompts/prompts.json\", 'r', encoding='utf-8') as file:\n",
    "    prompts = json.load(file)\n",
    "\n",
    "with open(\"preguntas/preguntas.json\", 'r') as file:\n",
    "    preguntas = json.load(file)['preguntas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función **\"ejecutar_prompt\"** debe implemenetar las llamadas correpondientes al API del LLM que se esté utilizando. En caso de que el LLM no disponga de un API, se debe contactar con lfsanchez@comillas.edu para averiguar la mejor forma de automatizar la toma de resultados del conjunto de preguntas de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_prompt(prompt, pregunta):\n",
    "    # Esta función debería ser modificada para realizar llamadas reales a APIs de modelos de lenguaje.\n",
    "    # La siguiente es una simulación de respuesta.\n",
    "    respuesta_simulada = \"Esto es una respuesta simulada.\"\n",
    "    justificacion_simulada = \"Esta es una justificación simulada.\"\n",
    "    return respuesta_simulada, justificacion_simulada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del API de OpenAI para chatGPT 3.5 el código se muestra a continuación, aunque el primer paso es instalar la librería que implementa las funciones de llamada a su API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que disponemos de la librería y de un API key generado desde un perfil de OpenAI, un código como el que se presenta a continuación permitiría utilizar ChatGPT 3.5 de manera programática.\n",
    "\n",
    "La implementación de la función **ejecutar_prompt** no es única. Lo único importante es que la función reciba el prompt y la consulta y la función devuelva el resultado y la justificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "def ejecutar_prompt(prompt, question):\n",
    "    # Descomentar la línea del api_key y utilizar la clave real\n",
    "    client = OpenAI(\n",
    "        #api_key=\"API KEY FOR OPEN AI\",\n",
    "    )\n",
    "    \n",
    "    # Reemplaza {} por la pregunta que debe ir en el prompt\n",
    "    for message in prompt:\n",
    "        if message['role'] == 'user' and \"{}\" in message['content']:\n",
    "            message['content'] = message['content'].replace(\"{}\", question)\n",
    "\n",
    "    # Invoca al modelo de lenguaje. Notar que se mete en un bucle infinito porque para poder interpreta los resultados\n",
    "    # la consulta debe devolver la salida en el formato que le pedimos. Si el formato no es correcto,\n",
    "    # volvemos a repetir la consulta\n",
    "    while True:\n",
    "        response = client.chat.completions.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=prompt,\n",
    "          max_tokens=150,\n",
    "          temperature=0.7\n",
    "        )\n",
    "        try:\n",
    "            respuesta, justificacion = response.choices[0].message.content.split(\"^\")\n",
    "            print(\"  Done\")\n",
    "            break\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "\n",
    "    return respuesta, justificacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, esta porción de código recorre todas las preguntas del fichero **\"preguntas.txt\"** y todos los prompts del fichero **\"prompts.json\"** e invoca a **\"ejecutar_prompt\"** para cada combinación. El código recoje la respuesta del LLM en forma de una tupla (respuesta, justificación) que se utiliza para evaluar el resultado de la entrega.\n",
    "\n",
    "Dependiendo de si el fichero **\"prompts.json\"** incluye un único prompt para todas las preguntas o diferentes prompts para cada uno de los bloques del examen, el código aplicará el prompt adecuado a cada pregunta en función del valor del campo **categoría** que especifica a qué bloque del exámen pertenece la pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pregunta: 0 Prompt: comun\n",
      "  Done\n",
      "- Pregunta: 1 Prompt: laboral\n",
      "not enough values to unpack (expected 2, got 1)\n",
      "not enough values to unpack (expected 2, got 1)\n",
      "  Done\n",
      "- Pregunta: 2 Prompt: civil_mercantil\n",
      "  Done\n",
      "- Pregunta: 3 Prompt: penal\n",
      "  Done\n",
      "- Pregunta: 4 Prompt: administrativa\n",
      "  Done\n"
     ]
    }
   ],
   "source": [
    "resultados = []\n",
    "\n",
    "if len(prompts[\"prompts\"])==1:\n",
    "    multi_prompt=False\n",
    "else:\n",
    "    multi_prompt=True\n",
    "\n",
    "for pregunta_id in range(len(preguntas)):\n",
    "    pregunta=preguntas[pregunta_id]\n",
    "    categoria=pregunta[\"categoria\"]\n",
    "    opciones_texto = \"\\n\".join([f\"{opcion['opcion']}: {opcion['texto']}\" for opcion in pregunta['opciones']])\n",
    "    pregunta_completa = f\"{pregunta['pregunta']}\\nOpciones:\\n{opciones_texto}\"\n",
    "    prompt_list=copy.deepcopy(prompts[\"prompts\"])\n",
    "\n",
    "    if multi_prompt==True and categoria in prompt_list.keys():\n",
    "        promptid=categoria\n",
    "        prompt=prompt_list[promptid]\n",
    "    else:\n",
    "        promptid=\"comun\"\n",
    "        prompt=prompt_list[\"comun\"]\n",
    "       \n",
    "    print(\"- Pregunta: {} Prompt: {}\".format(pregunta_id,promptid))\n",
    "    prompt=prompt_list[promptid]\n",
    "    respuesta, justificacion = ejecutar_prompt(prompt, pregunta_completa)\n",
    "    resultados.append({\n",
    "        \"pregunta\": pregunta_completa,\n",
    "        \"prompt\":promptid,\n",
    "        \"respuesta_simulada\": respuesta,\n",
    "        \"respuesta_correcta\": pregunta[\"respuesta_correcta\"],\n",
    "        \"justificacion_simulada\": justificacion\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las salidas se van almacenando en la variable **resultados** que va a permitir calcular las diferentes métricas de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pregunta': '¿Cuánto es 2 + 2?\\nOpciones:\\nA: 3\\nB: 4\\nC: 5',\n",
       "  'prompt': 'comun',\n",
       "  'respuesta_simulada': 'B',\n",
       "  'respuesta_correcta': 'B',\n",
       "  'justificacion_simulada': 'La respuesta correcta es 4, ya que al sumar 2 + 2 obtenemos como resultado 4.'},\n",
       " {'pregunta': '¿Cuánto es 5 - 3?\\nOpciones:\\nA: 2\\nB: 3\\nC: 4',\n",
       "  'prompt': 'laboral',\n",
       "  'respuesta_simulada': 'A',\n",
       "  'respuesta_correcta': 'A',\n",
       "  'justificacion_simulada': 'La resta de cinco menos tres es igual a dos.'},\n",
       " {'pregunta': '¿Cuánto  es 7 * 1?\\nOpciones:\\nA: 6\\nB: 7\\nC: 8',\n",
       "  'prompt': 'civil_mercantil',\n",
       "  'respuesta_simulada': 'B',\n",
       "  'respuesta_correcta': 'B',\n",
       "  'justificacion_simulada': 'La respuesta correcta es 7, ya que el resultado de multiplicar 7 por 1 es igual a 7.'},\n",
       " {'pregunta': '¿Cuánto  es 10 / 2?\\nOpciones:\\nA: 4\\nB: 5\\nC: 6',\n",
       "  'prompt': 'penal',\n",
       "  'respuesta_simulada': 'B',\n",
       "  'respuesta_correcta': 'B',\n",
       "  'justificacion_simulada': 'La respuesta correcta es 5, ya que 10 dividido entre 2 es igual a 5.'},\n",
       " {'pregunta': '¿Cuánto  es el resultado de 3 + 6?\\nOpciones:\\nA: 8\\nB: 9\\nC: 10',\n",
       "  'prompt': 'administrativa',\n",
       "  'respuesta_simulada': 'B',\n",
       "  'respuesta_correcta': 'B',\n",
       "  'justificacion_simulada': 'El resultado de sumar 3 + 6 es 9.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, esos resultados se vuelcan a un fichero de texto para que el tribunal pueda evaluar la justificación proporcionada por el LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los resultados en un archivo de texto\n",
    "with open(\"resultados.txt\", \"w\") as file:\n",
    "    for resultado in resultados:\n",
    "        file.write(f\"Pregunta: {resultado['pregunta']}\\n\")\n",
    "        file.write(f\"Respuesta correcta: {resultado['respuesta_correcta']}\\n\")\n",
    "        file.write(f\"Respuesta simulada: {resultado['respuesta_simulada']}\\n\")\n",
    "        file.write(f\"Justificación simulada: {resultado['justificacion_simulada']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí iría el cálculo de métricas basado en las respuestas simuladas.\n",
    "# En una implementación real, compararíamos las respuestas simuladas con las respuestas correctas\n",
    "# para calcular el número de aciertos y la puntuación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
